{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD+bZ0ZZ5Lsw+oKs9rRGsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2021-uam-4646/codealpha_task/blob/main/CREDIT_SCORING_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91675fc7"
      },
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Yeh cell data ko load karne ke liye hai. `load_data_safely` function file se data ka sample load karta hai taake memory kam use ho, aur sirf zaruri columns ko select karta hai jaisa ke `keep_cols` list mein bataya gaya hai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64401920",
        "outputId": "f4d988cc-2d0a-4dd1-bbb5-630ef19edea4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "## 1. Smart Data Loading ##\n",
        "def load_data_safely(file_path, sample_frac=0.3):\n",
        "    # Get column names without loading full file\n",
        "    cols = pd.read_csv(file_path, nrows=0).columns.tolist()\n",
        "\n",
        "    # Select relevant columns (modify as needed)\n",
        "    # Corrected column names based on the notebook state\n",
        "    keep_cols = ['disbursed_amount', 'asset_cost', 'ltv', 'manufacturer_id', 'Employment.Type', 'loan_default', 'Date.of.Birth']\n",
        "\n",
        "\n",
        "    # Load sampled data with selected columns\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, usecols=keep_cols).sample(frac=sample_frac, random_state=42)\n",
        "        print(f\"Loaded {len(data)} samples safely\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        # Fallback to smaller sample if failed\n",
        "        # Also update keep_cols for the fallback\n",
        "        return pd.read_csv(file_path, usecols=keep_cols, nrows=50000)\n",
        "\n",
        "# Step 1: Load data safely\n",
        "data = load_data_safely('train.csv', sample_frac=0.4)  # Adjust fraction as needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 93262 samples safely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e4f889"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Yeh cell data ko model training ke liye prepare karta hai. Ismein missing values ko hataana aur categorical features ko numbers mein badalna shamil hai. 'Date.of.Birth' column ko bhi drop kiya gaya hai kyunki woh numerical operations mein masla kar raha tha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85d041b6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "## 2. Memory-Friendly Preprocessing ##\n",
        "def simple_preprocess(df):\n",
        "    # Handle missing values\n",
        "    df = df.dropna().copy() # Work on a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Convert categoricals (simple approach)\n",
        "    # Updated column name for employment type\n",
        "    if 'Employment.Type' in df.columns:\n",
        "        df.loc[:, 'Employment.Type'] = df['Employment.Type'].astype('category').cat.codes\n",
        "\n",
        "    # Basic feature engineering\n",
        "    df.loc[:, 'loan_to_asset_ratio'] = df['disbursed_amount'] / (df['asset_cost'] + 1e-6)\n",
        "\n",
        "    # Drop 'Date.of.Birth' as it's causing errors in numerical calculations\n",
        "    if 'Date.of.Birth' in df.columns:\n",
        "        df = df.drop('Date.of.Birth', axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 2: Preprocess\n",
        "data = simple_preprocess(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e2862b"
      },
      "source": [
        "## 3. Data Preparation and Split\n",
        "\n",
        "Yeh cell data ko model training ke liye tayyar karta hai. Features (input) aur target variable (output) ko alag alag kiya jata hai, aur phir data ko training aur testing sets mein divide kiya jata hai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b88252d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 3: Prepare data\n",
        "X = data.drop('loan_default', axis=1)\n",
        "y = data['loan_default']\n",
        "\n",
        "# Step 4: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbf882e"
      },
      "source": [
        "## 4. Model Training\n",
        "\n",
        "Yeh cell machine learning model ko train karta hai. Ismein `GradientBoostingClassifier` use kiya gaya hai jo data par fit hota hai taake patterns seekh kar future predictions kar sake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "256f2867"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "## 3. Model Training ##\n",
        "def train_model(X, y):\n",
        "    # Lightweight yet effective model\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        subsample=0.8  # Adds robustness\n",
        "    )\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# Step 5: Train model\n",
        "model = train_model(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37c1914"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "Yeh cell trained model ko evaluate karta hai taake uski performance dekhi ja sake. Ismein predictions karna aur `classification_report` aur `roc_auc_score` jaisi metrics calculate karna shamil hai.\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "Yeh cell memory se variables ko remove karta hai taake resources free hon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8da2a94a",
        "outputId": "4b7ec627-dfac-4f94-cd92-ecd31e798675"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "## 4. Evaluate Model ##\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# Step 6: Evaluate\n",
        "evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# Clean up\n",
        "del data, X, y, X_train, X_test, y_train, y_test, model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      1.00      0.88     14131\n",
            "           1       0.00      0.00      0.00      3910\n",
            "\n",
            "    accuracy                           0.78     18041\n",
            "   macro avg       0.39      0.50      0.44     18041\n",
            "weighted avg       0.61      0.78      0.69     18041\n",
            "\n",
            "ROC AUC: 0.6071\n"
          ]
        }
      ]
    }
  ]
}